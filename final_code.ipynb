{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b662dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.13.1+cpu torchvision==0.14.1+cpu â€“f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install intel_extension_for_pytorch==1.13.100 -f https://developer.intel.com/ipex-whl-stable-cpu\n",
    "# !pip install intel-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e644959",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'diffusers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyttsx3\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableDiffusionPipeline\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableDiffusionXLImg2ImgPipeline\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'diffusers'"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import os\n",
    "import io\n",
    "import pyttsx3\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "from diffusers.utils import load_image\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from PIL import Image\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "\n",
    "if(\"result\" not in os.listdir(os.getcwd())):\n",
    "    os.makedirs(\"result\")\n",
    "\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "#if ipex installed run this or run the line below it\n",
    "text_to_image_pipeline = ipex.optimize(StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32))\n",
    "# text_to_image_pipeline = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)\n",
    "\n",
    "\n",
    "# def refine_image_via_api(base_image, refinement_prompt):\n",
    "#     image_to_image_pipeline = pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True)\n",
    "#     refined_image_data =image_to_image_pipeline( refinement_prompt , base_image )   # Replace with actual API request\n",
    "#     refined_image = Image.open(io.BytesIO(refined_image_data))\n",
    "#     return refined_image\n",
    "# Initialize the Diffusion refiner model and tokenizer for image refinement\n",
    "\n",
    "save_folder=\"./result\"\n",
    "\n",
    "def generate_story_extension(user_input, tokenizer, model_name, temperature=1.0, max_new_tokens=1000):\n",
    " \n",
    "\n",
    "  # Use user_input directly as string for pipeline\n",
    "  generator = pipeline(\"text-generation\", model=model_name, tokenizer=tokenizer)\n",
    "  story_extension = generator(user_input, max_new_tokens=max_new_tokens, num_return_sequences=1, temperature=temperature)[0]['generated_text'][len(user_input)+1:]\n",
    "\n",
    "  return story_extension.strip()\n",
    "\n",
    "# Load tokenizer and model (optional, using pipeline is more convenient here)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t1 = AutoTokenizer.from_pretrained(\"Intel/neural-chat-7b-v3-3\")\n",
    "m1= ipex.optimize(AutoModelForCausalLM.from_pretrained(\"Intel/neural-chat-7b-v3-3\"))\n",
    "# m1 = AutoModelForCausalLM.from_pretrained(\"Intel/neural-chat-7b-v3-3\")\n",
    "    \n",
    "def start_story():\n",
    "  list_of_response = []\n",
    "  updated_story = \"\"\n",
    "  base_story = \"\"\n",
    "\n",
    "  characters = input(\"Enter the name of characters along with their descriptions \\n\")\n",
    "  plot = input(\"Describe the plot in brief \\n\")\n",
    "  theme = input(\"Provide a theme of the story along with the setting \\n\")\n",
    "\n",
    "\n",
    "\n",
    "  user_input = f\"Theme: {theme} plot: {plot} characters: {characters}\"\n",
    "\n",
    "  prompt = f\"generate the complete story based on: input given by the user: \\n {user_input} of at least 800 words\"\n",
    "  base_story = generate_story_extension(prompt, t1, model_name=\"Intel/neural-chat-7b-v3-3\", temperature=1.0, max_new_tokens=1000)\n",
    "  updated_story = base_story\n",
    "  narrative_finished = False\n",
    "  print(\"base story starts:\",updated_story)\n",
    "  while not narrative_finished :\n",
    "      user_changed_input = input(\"If you are satisfied with the story, please type end or type out the changes you want in the form of a simple prompt\")\n",
    "      if user_changed_input.lower()==\"end\" :\n",
    "        narrative_finished = True\n",
    "        break\n",
    "      prompt_change = f\"generate the new story by making changes in \\n {updated_story} based on \\n {user_changed_input} of at least 800 words \"\n",
    "      updated_story =  generate_story_extension(prompt_change, t1, model_name=\"Intel/neural-chat-7b-v3-3\", temperature=1.0, max_new_tokens=1000)\n",
    "      print(updated_story)\n",
    "  return updated_story\n",
    "\n",
    "\n",
    "story=start_story()\n",
    "prompt_for_img_gen = f\"Based on the story:{story} add \\n delimiters to separate the story into at least 10  pivotal parts(each part represent a different pivotal chapter of story) and  at max 20 parts, where each pivotal part gives an illustrative desciption of 30 to 40 words about that part such that an image can be generated from the part and can be fed into a text to image model to show progressive story, try involving new setting or new characters in each part\"\n",
    "prompt_corpus = generate_story_extension(prompt_for_img_gen, t1, model_name=\"Intel/neural-chat-7b-v3-3\", temperature=1.0, max_new_tokens=1000)\n",
    "p = \"Generate an immersive visual representation capturing a pivotal scene from the story, featuring the central character(s) in a vivid setting that encapsulates the thematic essence of the narrative.\"\n",
    "print(prompt_corpus)\n",
    "li=prompt_corpus.split(\"\\n\")\n",
    "story_sequence=[]\n",
    "for x in li:\n",
    "  if 'Part' in x:\n",
    "    story_sequence.append(x)\n",
    "print(story_sequence)\n",
    "\n",
    "base_images = []\n",
    "for i in range(len(story_sequence)):\n",
    "    if(i>=1):\n",
    "      image = text_to_image_pipeline(story_sequence[i-1]+story_sequence[i]).images[0]  # Get the image tensor\n",
    "    else:\n",
    "      image = text_to_image_pipeline(story_sequence[i]).images[0]  # Get the image tensor\n",
    "    base_images.append(image)\n",
    "for i, image in enumerate(base_images):\n",
    "\n",
    "    file_path = os.path.join(save_folder, f\"base_image_{i}.png\")\n",
    "    \n",
    "    image.save(file_path)\n",
    "\n",
    "\n",
    "\n",
    "for i,image_path in enumerate(sorted(os.listdir(save_folder))):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.setProperty('rate', 300)  \n",
    "    voices = engine.getProperty('voices')\n",
    "    engine.setProperty('voice', voices[0].id)  # Use the first voice by default\n",
    "    if image_path.endswith(\".png\"):  \n",
    "        full_path = os.path.join(save_folder, image_path)\n",
    "        img = Image.open(full_path) \n",
    "        img.show()\n",
    "        text = story_sequence[i]\n",
    "        engine.say(text)\n",
    "        engine.runAndWait()\n",
    "\n",
    "\n",
    "\n",
    "# Refine images using Diffusion refiner\n",
    "refined_images = []\n",
    "for i, base_image in enumerate(base_images):\n",
    "    context = story_sequence[:i+1]  # Context includes previous story scenes\n",
    "    context_prompt = \" \".join(context)\n",
    "    refinement_prompt = f\"Refine the image to depict: {context_prompt}\"\n",
    "\n",
    "    refined_image = refine_image_via_api(base_image, refinement_prompt)\n",
    "    refined_images.append(refined_image)\n",
    "\n",
    "# Save or display the refined images\n",
    "# for i, image in enumerate(refined_images):\n",
    "#     # Convert tensor to PIL image\n",
    "\n",
    "#     # Save the image\n",
    "#     file_path = os.path.join(save_folder, f\"refined_image_{i}.png\")\n",
    "#     image.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d50ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
